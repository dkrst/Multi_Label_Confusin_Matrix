{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "409df7bc-80d3-4845-9264-72bf32a10f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mlcevaluator1 import mlcEvaluator1\n",
    "from mlcevaluator2 import mlcEvaluator2\n",
    "from mlctensor import mlcTensor\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965d74a3-0675-44bd-bc2e-751d737d8c61",
   "metadata": {},
   "source": [
    "## Data set\n",
    "Select data set by uncomenting either synthetic data set with 3 labels and 9 instances, or movie poster data set (true labels and predictions are loaded from a file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec4a6368-9365-4dce-adde-c2e73754d00f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7209, 18)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example GT and Prediction matrices\n",
    "'''\n",
    "gt=np.asarray([[1,1,0], [1,1,1], [0,0,0],\n",
    "               [1,0,0], [1,1,0], [0,0,0],\n",
    "               [1,0,0], [1,1,0], [1,1,0]])\n",
    "              \n",
    "pred=np.asarray([[1,1,0],[1,0,1],[0,0,0],\n",
    "                 [1,1,1], [1,1,1], [0,1,1],\n",
    "                 [0,1,1], [1,0,1], [0,0,1]])\n",
    "'''\n",
    "# Load GT and prediction from file\n",
    "gt=np.load('data/posters/gt.npy')\n",
    "gt.shape\n",
    "\n",
    "# Uncomment one set of predictions\n",
    "#\n",
    "#pred=np.load('data/posters/pred_t05.npy')\n",
    "pred=np.load('data/posters/pred_t09.npy')\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa8228c-6865-4a68-84fe-888bf7fd1d63",
   "metadata": {},
   "source": [
    "The equations for computing contribution of a single data instance $i$ to the confusion tensor have an implicit assumption that $\\lvert T_i\\rvert > 0$ and $\\lvert P_i\\rvert > 0$, i.e. that both true labels and predictions for the data instance $i$ have at least one label assigned. To cope with the scenarios where true labels or prediction has no labels assigned, an additional class is included in computing the confusion tensor. This label, *unknown* is added as last element of each $T_i$ and $P_i$ vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddf8fff-2599-42c9-b92a-d39ff923b000",
   "metadata": {},
   "source": [
    "## Multi-label Confusion Tensor\n",
    "Compute raw Multi-Label Confusion Tensor and normalized Recall and Precision Confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9006ae89-d146-45f8-932d-05641ca60e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "evalT = mlcTensor(gt, pred)\n",
    "MT = evalT.computeConfusionTensor()\n",
    "RT = evalT.getRecall()\n",
    "PT = evalT.getPrecision()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85d418f-cdb3-4d92-a58e-c8bd34185073",
   "metadata": {},
   "source": [
    "## Per-class Precision/Recall\n",
    "Per-class Recall and Precision are defined as:\n",
    "$$\n",
    "R(k) = \\frac{TP(k)}{TP(k)+FN(k)},\\qquad P(k) = \\frac{TP(k)}{TP(k)+FP(k)}\n",
    "$$\n",
    "where $k$ is the class index, $TP(k)$ stands for a number of correctly assigned labels, $FN(k)$ represents the number of cases where the relevant label $k$ was not assigned to an instance and $FP(k)$ is a number of instances with incorrectly assigned label $k$.\n",
    "$F_1$ score is the harmonic mean of the precision and recall:\n",
    "$$\n",
    "F_1(k) = \\frac{2*P(k)*R(k)}{P(k)+R(k)} = \\frac{2TP(k)}{2TP(k)+FP(k)+FN(k)}\n",
    "$$\n",
    "\n",
    "Recall for each class is represented by corresponding diagonal element in the recall matrix. Precision is represented by diagonal elements of the precision matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b079925a-0792-4d34-bf1f-aea650ba2751",
   "metadata": {},
   "outputs": [],
   "source": [
    "R=RT.diagonal()\n",
    "P=PT.diagonal()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28ce77f-3b80-455d-9dc1-2c88789a7f78",
   "metadata": {},
   "source": [
    "True positive values for each class $TP(k), k=1, ...,q$ are represented by diagonal elements of raw confusion tensor in both Recall and Precision matrices (diagonal elements in both matrices are exactly the same).\n",
    "\n",
    "False negative value $FN(k)$ for label $k$ can be computed as a sum of the corresponding row in the raw recall matrix (first elemet of Confusion Tensor) minus the value of the diagonal element, i.e. number of true positives for the same label.\n",
    "\n",
    "Similarly, False positive values $FP(k)$ are represented by the sum of the corresponding column in the raw Precision matrix (second elemet of Confusion Tensor) minus the value of the diagonal, i.e. number of true positives for the same label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7096688b-c2e2-47f1-84d9-09060b88a0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " k\t| R(k)\t P(k)\t F1(k)\n",
      "---------------------------------\n",
      " 0\t| 0.10\t 0.14\t| 0.12\n",
      " 1\t| 0.06\t 0.10\t| 0.08\n",
      " 2\t| 0.01\t 0.03\t| 0.01\n",
      " 3\t| 0.03\t 0.05\t| 0.04\n",
      " 4\t| 0.31\t 0.32\t| 0.31\n",
      " 5\t| 0.09\t 0.11\t| 0.10\n",
      " 6\t| 0.06\t 0.08\t| 0.07\n",
      " 7\t| 1.00\t 0.51\t| 0.67\n",
      " 8\t| 0.02\t 0.05\t| 0.03\n",
      " 9\t| 0.01\t 0.02\t| 0.02\n",
      "10\t| 0.00\t 0.00\t| 0.00\n",
      "11\t| 0.06\t 0.09\t| 0.07\n",
      "12\t| 0.00\t 0.00\t| 0.00\n",
      "13\t| 0.03\t 0.06\t| 0.04\n",
      "14\t| 0.15\t 0.16\t| 0.15\n",
      "15\t| 0.02\t 0.06\t| 0.03\n",
      "16\t| 0.11\t 0.12\t| 0.12\n",
      "17\t| 0.01\t 0.03\t| 0.01\n",
      "18\t| 0.00\t 0.00\t| 0.00\n"
     ]
    }
   ],
   "source": [
    "TP = MT[0,:,:].diagonal()\n",
    "FN = MT[0,:,:].sum(axis=1)-TP\n",
    "FP = MT[1,:,:].sum(axis=0)-TP\n",
    "\n",
    "F1 = 2*TP/(2*TP+FP+FN)\n",
    "\n",
    "print(' k\\t| R(k)\\t P(k)\\t F1(k)')\n",
    "print('---------------------------------')\n",
    "for k in range(R.shape[0]):\n",
    "    print('%2d\\t| %.2f\\t %.2f\\t| %.2f' %(k, R[k], P[k], F1[k]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3209bb-c4c6-4897-a8bf-4e0f38bda065",
   "metadata": {},
   "source": [
    "## Evaluating Classifier performance over all labels \n",
    "Let $\\boldsymbol{T}_i$ be the vector representing the set of true labels for data instance $i$ and $\\boldsymbol{P}_i$ be the vector of predicted labels for the same instance. Vectors $\\boldsymbol{T}_{i1}=\\boldsymbol{P}_{i1}=\\boldsymbol{T}_i\\cap \\boldsymbol{P}_i$ represents correctly predicted labels. $\\boldsymbol{T}_{i2}=\\boldsymbol{T}_i\\backslash \\boldsymbol{P}_i$ is a set of true labels not predicted by the classifier, while $\\boldsymbol{P}_{i2} = \\boldsymbol{P}_i\\backslash \\boldsymbol{T}_i$ represents incorrectly predicted labels. It is clear that $\\boldsymbol{T}_i = \\boldsymbol{T}_{i1} + \\boldsymbol{T}_{i1}$ and $\\boldsymbol{P}_i = \\boldsymbol{P}_{i1}+\\boldsymbol{P}_{i2}$.<br>\n",
    " - $TP(k)$ (True Positive) stands for a number of instances with correctly assigned label $k$\n",
    " - $FP(k)$ (False Positive) stands for a number of instances with incorrectly assigned label $k$\n",
    " - $FN(k)$ (False Negative) represents the number of cases where the relevant label $k$ was not assigned to an instance. \n",
    "\n",
    "Let $B(TP(k) , FP(k) , TN(k), FN(k))$ be some specific binary classification metric, $k = 1, ...,q$, where $q$ is the number of possible labels.\n",
    "\n",
    "Label-based classification metrics for a classificator can be obtained using either Macro-averaging or Micro-averaging approach.\n",
    "\n",
    "### Macro-averaging\n",
    "Macro-averaging averages over all  categories, thus giving each category equal weight<br>\n",
    "$B_{Macro} = \\frac{1}{q}\\sum\\limits_{k=1}^q B\\big[ TP(k), FP(k), TN(k), FN(k)\\big]$\n",
    "\n",
    "Macro-averaged Recall and Precision for a classifier can be computed as:<br>\n",
    "$R_{Macro} = \\frac{1}{q}\\sum\\limits_{k=1}^q \\frac{TP(k)}{TP(k)+FN(k)} = \\frac{1}{q}\\sum\\limits_{k=1}^q R(k)$\n",
    "\n",
    "$P_{Macro} = \\frac{1}{q}\\sum\\limits_{k=1}^q \\frac{TP(k)}{TP(k)+FP(k)} = \\frac{1}{q}\\sum\\limits_{k=1}^q P(k)$,\n",
    "\n",
    "$F_{1Macro} = \\frac{1}{q}\\sum\\limits_{k=1}^q \\frac{2TP(k)}{2TP(k)+FP(k)+FN(k)} = \\frac{1}{q}\\sum\\limits_{k=1}^qF_1(k)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5265c8d-6912-48df-af14-1b44e707e94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-averaged Classifier Recall:    [0.11]\n",
      "Macro-averaged Classifier Precision: [0.1]\n",
      "Macro-averaged Classifier F1 score:  [0.1]\n"
     ]
    }
   ],
   "source": [
    "q = R.shape\n",
    "\n",
    "RMacro = R.sum()/q\n",
    "PMacro = P.sum()/q\n",
    "F1Macro = F1.sum()/q\n",
    "\n",
    "print('Macro-averaged Classifier Recall:   ', RMacro.round(decimals=2))\n",
    "print('Macro-averaged Classifier Precision:', PMacro.round(decimals=2))\n",
    "print('Macro-averaged Classifier F1 score: ', F1Macro.round(decimals=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198f075a-248b-47be-83f5-cc9562ffde11",
   "metadata": {},
   "source": [
    "### Micro-averaging\n",
    "Macro-averaging averages over data instances, thus giving each sample equal weight<br>\n",
    "$B_{Micro} = B\\big[\\sum\\limits_{k=1}^q TP(k), \\sum\\limits_{k=1}^q FP(k), \\sum\\limits_{k=1}^q TN(k), \\sum\\limits_{k=1}^q FN(k)\\big]$\n",
    "\n",
    "Micro-averaged Recall and Precision for a classifier can be computed as:<br>\n",
    "$R_{Micro} = \\frac{\\sum\\limits_{k=1}^q TP(k)}{\\sum\\limits_{k=1}^q TP(k)+\\sum\\limits_{k=1}^q FN(k)}$\n",
    "\n",
    "$P_{Micro} = \\frac{\\sum\\limits_{k=1}^q TP(k)}{\\sum\\limits_{k=1}^q TP(k)+ \\sum\\limits_{k=1}^qFP(k)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96bcf45d-4676-4b6f-b719-6a67c2d8d7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-averaged Classifier Recall: 0.34\n",
      "Micro-averaged Classifier Precision: 0.33\n",
      "Micro-averaged Classifier F1 score: 0.33\n"
     ]
    }
   ],
   "source": [
    "TPs = TP.sum()\n",
    "FPs = FP.sum()\n",
    "FNs = FN.sum()\n",
    "RMicro = TPs/(TPs+FNs)\n",
    "PMicro = TPs/(TPs+FPs)\n",
    "F1Micro = 2*TPs/(2*TPs+FPs+FNs)\n",
    "\n",
    "print('Micro-averaged Classifier Recall:', RMicro.round(decimals=2))\n",
    "print('Micro-averaged Classifier Precision:', PMicro.round(decimals=2))\n",
    "print('Micro-averaged Classifier F1 score:', F1Micro.round(decimals=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
