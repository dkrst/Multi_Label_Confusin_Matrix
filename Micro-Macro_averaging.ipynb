{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "409df7bc-80d3-4845-9264-72bf32a10f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mlcevaluator1 import mlcEvaluator1\n",
    "from mlcevaluator2 import mlcEvaluator2\n",
    "from mlctensor import mlcTensor\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965d74a3-0675-44bd-bc2e-751d737d8c61",
   "metadata": {},
   "source": [
    "## Syntetic example\n",
    "Generate true and predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec4a6368-9365-4dce-adde-c2e73754d00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example GT and Prediction matrices\n",
    "gt=np.asarray([[1,1,0], [1,1,1], [0,0,0],\n",
    "               [1,0,0], [1,1,0], [0,0,0],\n",
    "               [1,0,0], [1,1,0], [1,1,0]])\n",
    "              \n",
    "pred=np.asarray([[1,1,0],[1,0,1],[0,0,0],\n",
    "                 [1,1,1], [1,1,1], [0,1,1],\n",
    "                 [0,1,1], [1,0,1], [0,0,1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa8228c-6865-4a68-84fe-888bf7fd1d63",
   "metadata": {},
   "source": [
    "The equations for computing contribution of a single data instance $i$ to the confusion tensor have an implicit assumption that $\\lvert T_i\\rvert > 0$ and $\\lvert P_i\\rvert > 0$, i.e. that both true labels and predictions for the data instance $i$ have at least one label assigned. To cope with the scenarios where true labels or prediction has no labels assigned, an additional class is included in computing the confusion tensor. This label, *unknown* is added as last element of each $T_i$ and $P_i$ vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddf8fff-2599-42c9-b92a-d39ff923b000",
   "metadata": {},
   "source": [
    "## Multi-label Confusion Tensor\n",
    "Compute raw Multi-Label Confusion Tensor and normalized Recall and Precision Confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9006ae89-d146-45f8-932d-05641ca60e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "evalT = mlcTensor(gt, pred)\n",
    "MT = evalT.computeConfusionTensor(unique=True)\n",
    "RT = evalT.getRecall()\n",
    "PT = evalT.getPrecision()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4b9885f-3035-4c7d-856c-0b2cb2b58efa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PT.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85d418f-cdb3-4d92-a58e-c8bd34185073",
   "metadata": {},
   "source": [
    "## Per-class Precision/Recall\n",
    "Per-class Recall and Precision are defined as:\n",
    "$$\n",
    "R(k) = \\frac{TP(k)}{TP(k)+FN(k)},\\qquad P(k) = \\frac{TP(k)}{TP(k)+FP(k)}\n",
    "$$\n",
    "where $k$ is the class index, $TP(k)$ stands for a number of correctly assigned labels, $FN(k)$ represents the number of cases where the relevant label $k$ was not assigned to an instance and $FP(k)$ is a number of instances with incorrectly assigned label $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0a792f8-aebd-4cc5-b910-200861694f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall values for each class:\n",
      "[0.71 0.4  1.   0.5 ]\n",
      "Precision values for each class:\n",
      "[1.   0.4  0.14 1.  ]\n"
     ]
    }
   ],
   "source": [
    "rT=RT.diagonal()\n",
    "pT=PT.diagonal()\n",
    "\n",
    "print('Recall values for each class:')\n",
    "print(rT.round(decimals=2))\n",
    "print('Precision values for each class:')\n",
    "print(pT.round(decimals=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3209bb-c4c6-4897-a8bf-4e0f38bda065",
   "metadata": {},
   "source": [
    "## Evaluating Classifier performance over all labels \n",
    "Let $\\boldsymbol{T}_i$ be the vector representing the set of true labels for data instance $i$ and $\\boldsymbol{P}_i$ be the vector of predicted labels for the same instance. Vectors $\\boldsymbol{T}_{i1}=\\boldsymbol{P}_{i1}=\\boldsymbol{T}_i\\cap \\boldsymbol{P}_i$ represents correctly predicted labels. $\\boldsymbol{T}_{i2}=\\boldsymbol{T}_i\\backslash \\boldsymbol{P}_i$ is a set of true labels not predicted by the classifier, while $\\boldsymbol{P}_{i2} = \\boldsymbol{P}_i\\backslash \\boldsymbol{T}_i$ represents incorrectly predicted labels. It is clear that $\\boldsymbol{T}_i = \\boldsymbol{T}_{i1} + \\boldsymbol{T}_{i1}$ and $\\boldsymbol{P}_i = \\boldsymbol{P}_{i1}+\\boldsymbol{P}_{i2}$.<br>\n",
    " - $TP(k)$ (True Positive) stands for a number of instances with correctly assigned label $k$\n",
    " - $FP(k)$ (False Positive) stands for a number of instances with incorrectly assigned label $k$\n",
    " - $FN(k)$ (False Negative) represents the number of cases where the relevant label $k$ was not assigned to an instance. \n",
    "\n",
    "Let $B(TP(k) , FP(k) , TN(k), FN(k))$ be some specific binary classification metric, $k = 1, ...,q$, where $q$ is the number of possible labels.\n",
    "\n",
    "Label-based classification metrics for a classificator can be obtained using either Macro-averaging or Micro-averaging approach.\n",
    "\n",
    "### Macro-averaging\n",
    "Macro-averaging averages over all  categories, thus giving each category equal weight<br>\n",
    "$B_{Macro} = \\frac{1}{q}\\sum\\limits_{k=1}^q B\\big[ TP(k), FP(k), TN(k), FN(k)\\big]$\n",
    "\n",
    "Macro-averaged Recall and Precision for a classifier can be computed as:<br>\n",
    "$R_{Macro} = \\frac{1}{q}\\sum\\limits_{k=1}^q \\frac{TP(k)}{TP(k)+FN(k)} = \\frac{1}{q}\\sum\\limits_{k=1}^q R(k)$\n",
    "\n",
    "$P_{Macro} = \\frac{1}{q}\\sum\\limits_{k=1}^q \\frac{TP(k)}{TP(k)+FP(k)} = \\frac{1}{q}\\sum\\limits_{k=1}^q P(k)$,\n",
    "\n",
    "where $R(k)$ and $P(k)$ are per-class Recall and Precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5265c8d-6912-48df-af14-1b44e707e94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-averaged Classifier Recall: 0.65\n",
      "Macro-averaged Classifier Precision: 0.64\n"
     ]
    }
   ],
   "source": [
    "RMacro = rT.sum()/rT.shape[0]\n",
    "PMacro = pT.sum()/pT.shape[0]\n",
    "\n",
    "print('Macro-averaged Classifier Recall:', RMacro.round(decimals=2))\n",
    "print('Macro-averaged Classifier Precision:', PMacro.round(decimals=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198f075a-248b-47be-83f5-cc9562ffde11",
   "metadata": {},
   "source": [
    "### Micro-averaging\n",
    "Macro-averaging averages over data instances, thus giving each sample equal weight<br>\n",
    "$B_{Micro} = B\\big[\\sum\\limits_{k=1}^q TP(k), \\sum\\limits_{k=1}^q FP(k), \\sum\\limits_{k=1}^q TN(k), \\sum\\limits_{k=1}^q FN(k)\\big]$\n",
    "\n",
    "Micro-averaged Recall and Precision for a classifier can be computed as:<br>\n",
    "$R_{Micro} = \\frac{\\sum\\limits_{k=1}^q TP(k)}{\\sum\\limits_{k=1}^q TP(k)+\\sum\\limits_{k=1}^q FN(k)}$\n",
    "\n",
    "$P_{Micro} = \\frac{\\sum\\limits_{k=1}^q TP(k)}{\\sum\\limits_{k=1}^q TP(k)+ \\sum\\limits_{k=1}^qFP(k)}$\n",
    "\n",
    "True positive values for each class $TP(k), k=1, ...,q$ are represented by diagonal elements of raw confusion tensor in both Recall and Precision matrices:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "451d778d-bcf8-49c4-b1cb-678222636ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5. 2. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "TP = MT[0,:,:].diagonal()\n",
    "print(TP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffba3bee-4500-44df-a0e1-a332d33d0cab",
   "metadata": {},
   "source": [
    "False negative value $FN(k)$ for label $k$ can be computed as a sum of the corresponding row in the Recall matrix (first elemet of Confusion Tensor) minus the value of the diagonal element, i.e. number of true positives for the same label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "515b0165-63ad-4e82-82fc-5a420267feda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 3. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "FN = MT[0,:,:].sum(axis=1) - MT[0,:,:].diagonal()\n",
    "print(FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382a4d2f-88d7-4351-a66c-1ee0a2e2185d",
   "metadata": {},
   "source": [
    "False positive values $FP(k)$ are represented by the sum of the corresponding column in the Precision matrix (second elemet of Confusion Tensor) minus the value of the diagonal, i.e. number of true positives for the same label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd88fe72-a523-4874-9b32-1d4ffc1d0d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 3. 6. 0.]\n"
     ]
    }
   ],
   "source": [
    "FP = MT[1,:,:].sum(axis=0) - MT[1,:,:].diagonal()\n",
    "print(FP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24833c9a-a7b8-4167-b28d-3f9ed829dd82",
   "metadata": {},
   "source": [
    "Micro-averaged Recall and Confusion (averaged over all samples) can be easily computed using the above values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96bcf45d-4676-4b6f-b719-6a67c2d8d7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-averaged Classifier Recall: 0.6\n",
      "Micro-averaged Classifier Precision: 0.5\n"
     ]
    }
   ],
   "source": [
    "RMicro = TP.sum()/(TP.sum()+FN.sum())\n",
    "PMicro = TP.sum()/(TP.sum()+FP.sum())\n",
    "\n",
    "print('Micro-averaged Classifier Recall:', RMicro.round(decimals=2))\n",
    "print('Micro-averaged Classifier Precision:', PMicro.round(decimals=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b3e1c1-f16e-42ea-a574-6ecb8078b8b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be1b9ee-e59d-466a-a1c5-eba8a837545e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
